{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Face Recognition _ Detector de Mascara Covid-19.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMdprIbJx8o0GVbC1XAOmim",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gorpo/Bases-python/blob/master/Face_Recognition___Detector_de_Mascara_Covid_19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqKmvgdHh9i2",
        "colab_type": "text"
      },
      "source": [
        "# Face Recognition | Detector de Mascara Covid-19\n",
        "Considerações: Este script com a camera irá rodar somente no navegador Google Chrome, execute as celulas em ordem e em caso de travamentos pare a maquina por total. Este script aceita sua WebCam Built-in ou uma WebCam tradicional. Caso queira usar seu celular como WebCam aconselho uso do [IvCam para Android](https://www.e2esoft.com/ivcam/). Baixe o cliente em seu celular e o servidor em seu computador depois rode este script.\n",
        "                                                     @GorpoOrko | 05/2020 | Manicomio Python Project\n",
        "                                                     ![demonstração](https://github.com/gorpo/Face-Recognition-Detector-de-Mascara-Python-Covid-19/raw/master/exemplos/Screenshot_1.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7RCISnFiIjP",
        "colab_type": "text"
      },
      "source": [
        "# Downloads\n",
        "Execute a celula de Download e aguarde o download até o final, uma mensagem será exibida informando que todos seus arquivos foram baixados e descompactados com sucesso, estes arquivos incluem um dataset de imagens positivas e negativas com mais de 500 imagens em cada categoria, fique a vontade para inserir mais imagens na pasta /dataset caso ache necessário. Após a conclusão dos dowloads os arquivos seráo automaticamente descompactados, aguarde a mensagem de confirmação:  \"Todos arquivos baixados e descompactados com exito inicie o trainer!\"\n",
        "Após esta mensagem inicie o Trainer!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlmN1UMVia6o",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Inicie os Downloads.\n",
        "from google.colab import output\n",
        "#baixa o dataset\n",
        "!wget https://archive.org/download/dataset_202005/dataset.zip\n",
        "output.clear()\n",
        "#baixa os arquivos de deteccao\n",
        "!wget https://archive.org/download/dataset_202005/detectores.zip\n",
        "output.clear()\n",
        "#baixa as imagens de exemplo para testes\n",
        "!wget https://archive.org/download/dataset_202005/exemplos.zip\n",
        "output.clear()\n",
        "#dizipando os arquivos baixados\n",
        "!unzip dataset.zip\n",
        "output.clear()\n",
        "!unzip detectores.zip\n",
        "output.clear()\n",
        "!unzip exemplos.zip\n",
        "output.clear()\n",
        "!rm dataset.zip\n",
        "!rm detectores.zip\n",
        "!rm exemplos.zip\n",
        "#mensagem avisando que o processo acabou\n",
        "output.clear()\n",
        "!echo Todos arquivos baixados e descompactados com exito inicie o trainer!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZKdCTWdiImT",
        "colab_type": "text"
      },
      "source": [
        "# Trainer\n",
        "O arquivo trainer irá treinar um 'modelo' de predicção(acertividade) com base nas imagens contidas na pasta dataset previamente baixada, quanto mais imagens 'positivas'(com mascara) e quanto mais imagens 'negativas'(sem mascara) melhor será o resultado, este dataset de imagens conta com mais de 500 imagens em cada categoria. Agora basta executar o Trainer mas antes selecione a quantidade de vezes que ele irá repetir os treinos, quanto maior melhor será a predicção(acertividade). No final do processo será exibido um grafico mostrando a qualidade do seu Trainer, caso queira setar um valor maior de treinos altere o código desta celula."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGN6CNGqIY4j",
        "colab_type": "text"
      },
      "source": [
        "Selecione a quantidade de treinos e inicie o Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2xBkVqng8fu",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "quantidade_treinos = 10 #@param {type:\"slider\", min:10, max:100, step:1}\n",
        "from google.colab import output\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "# inicialize a taxa de aprendizado inicial, número de épocas para treinamento e tamanho do lote\n",
        "inicia_apredizado = 1e-4\n",
        "#quantidade_treinos = 10\n",
        "tamanho_lote = 32 #batch_size\n",
        "# pegue a lista de imagens em nosso diretório de conjunto de dados e inicialize a lista de dados (ou seja, imagens) e imagens de classe\n",
        "print(\"[INFO] carregando imagens...\")\n",
        "caminho_imagens = list(paths.list_images(\"dataset\"))\n",
        "dados = []\n",
        "labels = []\n",
        "\n",
        "# loop sobre os caminhos da imagem\n",
        "for imagePath in caminho_imagens:\n",
        "\t# extrai o rótulo da classe do nome do arquivo\n",
        "\tlabel = imagePath.split(os.path.sep)[-2]\n",
        "\t# carrega a imagem de entrada (224x224) e pré-processa\n",
        "\timagem = load_img(imagePath, target_size=(224, 224))\n",
        "\timagem = img_to_array(imagem)\n",
        "\timagem = preprocess_input(imagem)\n",
        "\t# atualiza as listas de dados e etiquetas, respectivamente\n",
        "\tdados.append(imagem)\n",
        "\tlabels.append(label)\n",
        "\n",
        "# converte os dados e rótulos em matrizes NumPy\n",
        "dados = np.array(dados, dtype=\"float32\")\n",
        "labels = np.array(labels)\n",
        "\n",
        "# executar codificação one-hot nas etiquetas\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "labels = to_categorical(labels)\n",
        "\n",
        "# particione os dados em divisões de treinamento e teste usando 75% dos dados para treinamento e os 25% restantes para teste\n",
        "(trainX, testX, trainY, testY) = train_test_split(dados, labels, test_size=0.20, stratify=labels, random_state=42)\n",
        "\n",
        "# construir o gerador de imagens de treinamento para aumento de dados\n",
        "gerador = ImageDataGenerator(rotation_range=20, zoom_range=0.15, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15, horizontal_flip=True, fill_mode=\"nearest\")\n",
        "\n",
        "# carregar a rede MobileNetV2, garantindo que os conjuntos de camadas FC principais sejam deixados de lado\n",
        "baseModel = MobileNetV2(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "# construa a cabeça do modelo que será colocado em cima do modelo base\n",
        "modelo_cabeca = baseModel.output\n",
        "modelo_cabeca = AveragePooling2D(pool_size=(7, 7))(modelo_cabeca)\n",
        "modelo_cabeca = Flatten(name=\"flatten\")(modelo_cabeca)\n",
        "modelo_cabeca = Dense(128, activation=\"relu\")(modelo_cabeca)\n",
        "modelo_cabeca = Dropout(0.5)(modelo_cabeca)\n",
        "modelo_cabeca = Dense(2, activation=\"softmax\")(modelo_cabeca)\n",
        "\n",
        "# coloque o modelo -FACE RECOGNITION- principal sobre o modelo base (este se tornará o modelo real que iremos treinar)\n",
        "modelo = Model(inputs=baseModel.input, outputs=modelo_cabeca)\n",
        "\n",
        "# percorre todas as camadas no modelo base e as congela para que elas * não * sejam atualizadas durante o primeiro processo de treinamento\n",
        "for layer in baseModel.layers:\n",
        "\tlayer.trainable = False\n",
        "\n",
        "# compile nosso modelo\n",
        "print(\"[INFO] compilando o modelo...\")\n",
        "otimizador = Adam(lr=inicia_apredizado, decay=inicia_apredizado / quantidade_treinos)\n",
        "modelo.compile(loss=\"binary_crossentropy\", optimizer=otimizador, metrics=[\"accuracy\"])\n",
        "\n",
        "# treinar a face\n",
        "print(\"[INFO] iniciando treinamento das faces...\")\n",
        "cabeca = modelo.fit(gerador.flow(trainX, trainY, batch_size=tamanho_lote), steps_per_epoch=len(trainX) // tamanho_lote, validation_data=(testX, testY), validation_steps=len(testX) // tamanho_lote, epochs=quantidade_treinos)\n",
        "output.clear()\n",
        "# faça previsões(predicções) no conjunto de testes\n",
        "print(\"[INFO] avaliação de rede neural...\")\n",
        "predIdxs = modelo.predict(testX, batch_size=tamanho_lote)\n",
        "# para cada imagem no conjunto de testes, precisamos encontrar o índice do rótulo com a maior probabilidade prevista correspondente\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "# mostrar um relatório de classificação\n",
        "print(classification_report(testY.argmax(axis=1), predIdxs,\ttarget_names=lb.classes_))\n",
        "# salve o modelo\n",
        "print(\"[INFO] salvando o modelo detector de mascara...\")\n",
        "modelo.save(\"detectores/mask_detector.model\", save_format=\"h5\")\n",
        "# traçar a perda e a precisão do treinamento\n",
        "N = quantidade_treinos\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), cabeca.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), cabeca.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), cabeca.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), cabeca.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig(\"detectores/plot.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJQ9VAB2mJzb",
        "colab_type": "text"
      },
      "source": [
        "# Analise de Imagens\n",
        "A células abaixo fazem a inserção das imagens e a análise em imagens estaticas verificando a acertividade em reconhecer o uso ou não da máscara em pessoas. Este script precisa da inserção de uma imagem via link ou upload, ou seja, basta adicionar a url de uma imagem que tenha o final .jpg ou .png conforme \n",
        "[exemplo](https://i.imgur.com/heVp6Jj.jpg) ou fazer o upload da sua imagem na proxima célula, lembre que só pode ser usada uma célula por vez. Após inserido link da imagem ou feito upload de uma imagem do seu computador basta iniciar a célula \"\"Analise de Imagens\"\" para iniciar o reconhecimento facial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOFb2aBlIPQJ",
        "colab_type": "text"
      },
      "source": [
        " **Upload da imagem via Link:**\n",
        "<br>Insira um link válido com final .jpg ou .png, em caso de erros  use o sistema de upload de imagens na celula abaixo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9Zk7FK2ExeP",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#SISTEMA DE ANALISE DE IMAGENS COM MASCARA COVID19\n",
        "#download da imagem a ser analisada:\n",
        "\n",
        "url = \"https://github.com/gorpo/Face-Recognition-Detector-de-Mascara-Python-Covid-19/blob/master/exemplos/1.png\" #@param {type:\"string\"}\n",
        "\n",
        "print(url)\n",
        "import requests\n",
        "import time\n",
        "\n",
        "#url = input()\n",
        "r = requests.get(url)\n",
        "with open('exemplos/file.png', 'wb') as f:\n",
        "    f.write(r.content)\n",
        "print('Download da imagem concluido com sucesso, inicie o processo de análise...')\t\t\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O-jpJeLIS4a",
        "colab_type": "text"
      },
      "source": [
        " **Upload de imagens do seu computador:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYhAFrUsFUvR",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown Rode esta célula e no botão 'Escolher arquivos', selecione a imagem que deseja fazer o upload.\n",
        "\n",
        "#faça upload da sua imagem do computador\n",
        "\n",
        "from google.colab import files\n",
        "import shutil\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  print('[INFO] Usuario fez upload do arquivo \"{name}\" com tamanho {length} bytes\\n[INFO] Imagem salva na pasta exemplos/file.png\\n[INFO] Upload concluido com sucesso rode a Analise de Imagens.'.format(name=fn, length=len(uploaded[fn])))\n",
        "shutil.move(fn, 'exemplos/file.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqs83nztIjxi",
        "colab_type": "text"
      },
      "source": [
        " **Análise de Imagens**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a_x7QABmSJa",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown Após ter feito upload da sua imagem com sucesso basta rodar esta célula para realizar o sistema de reconhecimento de imagens de pessoas com ou sem máscara!\n",
        "\n",
        "\n",
        "#inicio do reconhecimento facial e de mascaras:\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "\n",
        "try:\n",
        "\t# carregar nosso modelo de detector de rosto serializado a partir do disco\n",
        "\tprint(\"[INFO] carregando modelo detector de faces...\")\n",
        "\tprototxtPath = os.path.sep.join([\"detectores/deploy.prototxt\"])\n",
        "\tweightsPath = os.path.sep.join(\t[\"detectores/res10_300x300_ssd_iter_140000.caffemodel\"])\n",
        "\tnet = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
        "\t# carrega o detector de mascara facialload the face mask detector model from disk\n",
        "\tprint(\"[INFO] carregando detector de mascara facial...\")\n",
        "\tmodelo = load_model(\"detectores/mask_detector.model\")\n",
        "\n",
        "\t# carregue a imagem, clone-a e pegue as dimensões espaciais da imagem\n",
        "\timagem = cv2.imread('exemplos/file.png')\n",
        "\torigem = imagem.copy()\n",
        "\t(h, w) = imagem.shape[:2]\n",
        "\n",
        "\t# construir um blob a partir da imagem\n",
        "\tblob = cv2.dnn.blobFromImage(imagem, 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
        "\t# passe o blob pela rede e obtenha as detecções de rosto\n",
        "\tprint(\"[INFO] computando deteccoes faciais...\")\n",
        "\tnet.setInput(blob)\n",
        "\tdeteccoes = net.forward()\n",
        "\n",
        "\n",
        "\t# loop sobre as detecçoes\n",
        "\tfor i in range(0, deteccoes.shape[2]):\n",
        "\t\t# extrair a confiança (ou seja, probabilidade) associada à detecção\n",
        "\t\tconfianca = deteccoes[0, 0, i, 2]\n",
        "\t\t# filtrar detecções fracas, garantindo que a confiança seja maior que a confiança mínima\n",
        "\t\tif confianca > 0.5:\n",
        "\t\t\t# calcular as coordenadas (x, y) da caixa delimitadora para o objeto\n",
        "\t\t\tbox = deteccoes[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\t\t\t# verifique se as caixas delimitadoras estão dentro das dimensões do quadro\n",
        "\t\t\t(startX, startY) = (max(0, startX), max(0, startY))\n",
        "\t\t\t(endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
        "\t\t\t# extrai o ROI da face, converte-o de pedido de canal BGR para RGB, redimensione-o para 224x224 e pré-processe\n",
        "\t\t\tface = imagem[startY:endY, startX:endX]\n",
        "\t\t\tface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
        "\t\t\tface = cv2.resize(face, (224, 224))\n",
        "\t\t\tface = img_to_array(face)\n",
        "\t\t\tface = preprocess_input(face)\n",
        "\t\t\tface = np.expand_dims(face, axis=0)\n",
        "\t\t\t# passe o rosto pelo modelo para determinar se o rosto tem uma máscara ou não\n",
        "\t\t\t(mask, withoutMask) = modelo.predict(face)[0]\n",
        "\t\t\t# determine a label e a cor da classe que usaremos para desenhar a caixa delimitadora e o texto\n",
        "\t\t\tlabel = \"com mascara\" if mask > withoutMask else \"sem mascara\"\n",
        "\t\t\tcolor = (0, 255, 0) if label == \"com mascara\" else (0, 0, 255)\n",
        "\t\t\t# inclua a probabilidade na label\n",
        "\t\t\tlabel = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
        "\t\t\t# exibir a label e o retângulo da caixa delimitadora no quadro de saída\n",
        "\t\t\tcv2.putText(imagem, label, (startX, startY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
        "\t\t\tcv2.rectangle(imagem, (startX, startY), (endX, endY), color, 2)\n",
        "\n",
        "\t# Mostra a imagem final\n",
        "\tcv2_imshow(imagem)\n",
        "\tcv2.waitKey(0)\n",
        "except:\n",
        "\tprint('[ATENÇÃO] Ocorreu um erro ao processar sua imagem.\\n[MOTIVO] Mesmo ela tendo o final .jpg/.png ela não tinha as propriedades necessarias, devido ter vindo de uma fonte não confiavel.\\n[CORREÇÃO] Aconselho upload de suas imagens para analise no https://i.imgur.com/, insira o link da imagem upada com final .jpg ou .png') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1vwIWmVmOBZ",
        "colab_type": "text"
      },
      "source": [
        "# Analise com WebCam\n",
        "O sistema de análise de dados com a WebCam terá sua precisão com variação na qualidade da camera e da luz ambiente, este script roda tanto com uma camera local(built-in) como uma camera de celular, para usar seu celular como camera via PC aconselho o uso do [IvCam para Android](https://www.e2esoft.com/ivcam/). Baixe o cliente em seu celular e o servidor em seu computador depois rode este script. Porém caso queira usar direto seu telefone como camera, este script roda tranquilamente no navegador de celulares Google Chrome!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6l1_YQAmS33",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Inicie esta célula para fazer analise de dados via webcam\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import io\n",
        "import cv2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "from imutils.video import VideoStream\n",
        "import imutils\n",
        "import time\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "def VideoCapture():\n",
        "#inicio do codigo padrão para usar a camera no google colab conforme documentação\n",
        "  js = Javascript('''\n",
        "    async function create(){\n",
        "      div = document.createElement('div');\n",
        "      document.body.appendChild(div);\n",
        "      video = document.createElement('video');\n",
        "      video.setAttribute('playsinline', '');\n",
        "      div.appendChild(video);\n",
        "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "      canvas =  document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      div_out = document.createElement('div');\n",
        "      document.body.appendChild(div_out);\n",
        "      img = document.createElement('img');\n",
        "      div_out.appendChild(img);\n",
        "    }\n",
        "\n",
        "    async function capture(){\n",
        "        return await new Promise(function(resolve, reject){\n",
        "            pendingResolve = resolve;\n",
        "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "            result = canvas.toDataURL('image/jpeg', 0.8);\n",
        "            pendingResolve(result);\n",
        "        })\n",
        "    }\n",
        "    function showimg(imgb64){\n",
        "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
        "    }''')\n",
        "  display(js)\n",
        "def byte2image(byte):\n",
        "  jpeg = b64decode(byte.split(',')[1])\n",
        "  im = Image.open(io.BytesIO(jpeg))\n",
        "  return np.array(im)\n",
        "def image2byte(image):\n",
        "  image = Image.fromarray(image)\n",
        "  buffer = io.BytesIO()\n",
        "  image.save(buffer, 'jpeg')\n",
        "  buffer.seek(0)\n",
        "  x = b64encode(buffer.read()).decode('utf-8')\n",
        "  return x\n",
        "VideoCapture()\n",
        "eval_js('create()')  \n",
        "#final do codigo padrao para uso de camera no google colab, para chamar a camera no resto do script usaremos:\n",
        "#byte = eval_js('capture()')\n",
        "#imagem = byte2image(byte)\n",
        "#onde imagem ira retornar nossa imagem ja convertida para bytes\n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "try:\n",
        "  def deteccao(frame, faceNet, maskNet):\n",
        "    # pegue as dimensões do quadro e construa um blob a partir dele\n",
        "    (h, w) = frame.shape[:2]\n",
        "    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300),(104.0, 177.0, 123.0))\n",
        "    # passe o blob pela rede e obtenha as detecções de rosto\n",
        "    faceNet.setInput(blob)\n",
        "    detections = faceNet.forward()\n",
        "    # inicialize nossa lista de rostos, seus locais correspondentes e a lista de previsões da nossa rede de máscaras faciais\n",
        "    faces = []\n",
        "    locs = []\n",
        "    preds = []\n",
        "    # loop nas detecções\n",
        "    for i in range(0, detections.shape[2]):\n",
        "      # extrair a confiança (ou seja, probabilidade) associada à detecção\n",
        "      confidence = detections[0, 0, i, 2]\n",
        "      # filtrar detecções fracas, garantindo que a confiança seja maior que a confiança mínima\n",
        "      if confidence > 0.5:\n",
        "        # calcular as coordenadas (x, y) da caixa delimitadora para o objeto\n",
        "        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "        (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "        # verifique se as caixas delimitadoras estão dentro das dimensões do quadro\n",
        "        (startX, startY) = (max(0, startX), max(0, startY))\n",
        "        (endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
        "        # extrai o ROI da face, converte-o de pedido de canal BGR para RGB, redimensione-o para 224x224 e pré-processe-o\n",
        "        face = frame[startY:endY, startX:endX]\n",
        "        face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
        "        face = cv2.resize(face, (224, 224))\n",
        "        face = img_to_array(face)\n",
        "        face = preprocess_input(face)\n",
        "        face = np.expand_dims(face, axis=0)\n",
        "        # adicione as caixas de rosto e delimitadoras às respectivas listas\n",
        "        faces.append(face)\n",
        "        locs.append((startX, startY, endX, endY))\n",
        "    # apenas faça previsões se pelo menos uma face for detectada\n",
        "    if len(faces) > 0:\n",
        "      # para uma inferência mais rápida, faremos previsões de lote em * todos * rostos ao mesmo tempo, em vez de previsões um por um no loop `for` acima\n",
        "      preds = maskNet.predict(faces)\n",
        "    # retorna duas tuplas dos locais de face e seus locais correspondentes\n",
        "    return (locs, preds)\n",
        "\n",
        "\n",
        "  # carregar nosso modelo de detector de rosto serializado a partir do disco\n",
        "  print(\"[INFO] carregando modelo de detector de rosto ...\")\n",
        "  prototxtPath = os.path.sep.join([\"detectores/deploy.prototxt\"])\n",
        "  weightsPath = os.path.sep.join(\t[\"detectores/res10_300x300_ssd_iter_140000.caffemodel\"])\n",
        "  faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
        "  # carregar o modelo do detector de máscara facial a partir do disco\n",
        "  print(\"[INFO] carregando modelo de detector de máscara facial ...\")\n",
        "  maskNet = load_model(\"detectores/mask_detector.model\")\n",
        "  # inicialize o fluxo de vídeo e permita que o sensor da câmera capte imagens\n",
        "  print(\"[INFO] iniciando video...\")\n",
        "  print(\"[INFO] iniciando reconhecimento facial...\")\n",
        "  #mude o valor da camera para 0 caso a camera seja built-in ou  troque pelo ip de sua camera-->> 'http://192.168.0.4:4747/mjpegfeed'\n",
        "\n",
        "  # loop sobre o vídeo\n",
        "  while True:\n",
        "\n",
        "    # pegue o quadro do fluxo de vídeo encadeado e redimensione-o para ter uma largura máxima de 400 pixels\n",
        "    #linha 130 e 131 sao o retorno da camera(imagem em video)\n",
        "    byte = eval_js('capture()')\n",
        "    imagem = byte2image(byte)\n",
        "\n",
        "    #imagem = video.read()\n",
        "    imagem = imutils.resize(imagem, width=400)\n",
        "    # detectar rostos no quadro e determinar se eles estão usando uma máscara facial ou não\n",
        "\n",
        "    (locs, preds) = deteccao(imagem, faceNet, maskNet)\n",
        "    # circula sobre os locais de face detectados e seus locais correspondentes\n",
        "    for (box, pred) in zip(locs, preds):\n",
        "      # descompacte a caixa delimitadora e as previsões\n",
        "      (startX, startY, endX, endY) = box\n",
        "      (mask, withoutMask) = pred\n",
        "      # determine o rótulo e a cor da classe que usaremos para desenhar a caixa delimitadora e o texto\n",
        "      label = \"com mascara\" if mask > withoutMask else \"sem mascara\"\n",
        "      color = (0, 255, 0) if label == \"com mascara\" else (0, 0, 255)\n",
        "      # inclui a probabilidade em uma label\n",
        "      label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
        "      # exibe os textos da label no frame\n",
        "      cv2.putText(imagem, label, (startX, startY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
        "      cv2.rectangle(imagem, (startX, startY), (endX, endY), color, 2)\n",
        "\n",
        "    # mostra o video\n",
        "    eval_js('showimg(\"{}\")'.format(image2byte(imagem)))\n",
        "except:\n",
        "  print('Algum erro ocorreu no reconhecimento tente novamente!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUsKshgPK2sE",
        "colab_type": "text"
      },
      "source": [
        "![@GorpoOrko 2020 | tcxsproject.com.br](https://raw.githubusercontent.com/gorpo/Manicomio-Boot-Theme/master/manicomio/boot.png)"
      ]
    }
  ]
}